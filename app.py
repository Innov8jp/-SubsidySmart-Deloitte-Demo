# DeloitteSmartâ„¢ AI Assistant â€“ Full Version (Enhanced Multi-Doc Summarizer, Persistent Chat, Feedback, Downloadable Report - Basic Chunking)

import streamlit as st
import openai
import fitzÂ  # PyMuPDF
import json
from datetime import datetime
from openai import OpenAIError
import os
from io import BytesIO
import base64
import re # Import regular expressions for splitting

# --- CONFIGURATION - MUST BE FIRST ---
# Set page configuration for the Streamlit app
st.set_page_config(
Â  Â  page_title="DeloitteSmartâ„¢ - AI Assistant",
Â  Â  page_icon=":moneybag:",
Â  Â  layout="wide",
Â  Â  initial_sidebar_state="expanded"
)

# --- CONSTANTS ---
# Maximum approximate characters per chunk for splitting large documents
# This is a heuristic; actual token count is model-dependent (1 token ~ 4 chars for English)
# GPT-3.5-turbo often has a 16k token limit, so ~14000 chars leaves room for prompt/response
MAX_CHUNK_CHARS = 14000
# Smaller chunk size for splitting strategy
SPLIT_CHUNK_SIZE = 1000

# --- LANGUAGE TOGGLE AND TRANSLATION FUNCTION ---
# Allow users to select language via a radio button in the sidebar
language = st.sidebar.radio("ğŸŒ Language / è¨€èª", ["English", "æ—¥æœ¬èª"], index=0, key="language_select")

# Dictionary containing English to Japanese translations
def get_translation(english_text):
Â  Â  translations = {
Â  Â  Â  Â  "DeloitteSmartâ„¢ - AI Assistant": "DeloitteSmartâ„¢ - AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
Â  Â  Â  Â  "Faster, Smarter Decisions": "ã‚ˆã‚Šé€Ÿãã€ã‚ˆã‚Šã‚¹ãƒãƒ¼ãƒˆãªæ„æ€æ±ºå®š",
Â  Â  Â  Â  "ğŸ“ Upload Documents (PDF, TXT)": "ğŸ“ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF, TXT)",
Â  Â  Â  Â  "ğŸ“„ Summaries & Smart Questions": "ğŸ“„ è¦ç´„ã¨ã‚¹ãƒãƒ¼ãƒˆãªè³ªå•",
Â  Â  Â  Â  "ğŸ—‚ï¸": "ğŸ—‚ï¸",
Â  Â  Â  Â  "---": "---",
Â  Â  Â  Â  "ğŸ—£ï¸ Ask Questions Based on the Documents": "ğŸ—£ï¸ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã™ã‚‹",
Â  Â  Â  Â  "Ask anything about the uploaded documents...": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¤ã„ã¦ä½•ã§ã‚‚è³ªå•ã—ã¦ãã ã•ã„...",
Â  Â  Â  Â  "ğŸ’¬ Chat History": "ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå±¥æ­´",
Â  Â  Â  Â  "User": "ãƒ¦ãƒ¼ã‚¶ãƒ¼",
Â  Â  Â  Â  "Assistant": "ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
Â  Â  Â  Â  "â¬‡ï¸ Download Report": "â¬‡ï¸ ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
Â  Â  Â  Â  "ğŸ“ Feedback": "ğŸ“ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯",
Â  Â  Â  Â  "Share your feedback to help us improve:": "æ”¹å–„ã®ãŸã‚ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ãŠèã‹ã›ãã ã•ã„:",
Â  Â  Â  Â  "Submit Feedback": "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’é€ä¿¡",
Â  Â  Â  Â  "Thank you for your feedback!": "ã”æ„è¦‹ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼",
Â  Â  Â  Â  "Please enter your feedback.": "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "ğŸ“¬ Submitted Feedback": "ğŸ“¬ é€ä¿¡ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯",
Â  Â  Â  Â  "Timestamp": "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—",
Â  Â  Â  Â  "PDF extraction error for": "ã®PDFæŠ½å‡ºã‚¨ãƒ©ãƒ¼:",
Â  Â  Â  Â  "Summary generation error for": "ã®è¦ç´„ç”Ÿæˆã‚¨ãƒ©ãƒ¼:",
Â  Â  Â  Â  "Error during chat completion:": "ãƒãƒ£ãƒƒãƒˆå®Œäº†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:",
        "Error during document processing for": "ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼:",
Â  Â  Â  Â  "You are a highly trained consultant. Summarize the following content and generate 5 smart questions to ask the client.": "ã‚ãªãŸã¯é«˜åº¦ãªè¨“ç·´ã‚’å—ã‘ãŸã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®å†…å®¹ã‚’è¦ç´„ã—ã€ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å°‹ã­ã‚‹ã¹ã5ã¤ã®ã‚¹ãƒãƒ¼ãƒˆãªè³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "Document:": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:",
        "Documents:": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:", # Added for chat prompt context
Â  Â  Â  Â  "You are an AI assistant specialized in summarizing and extracting smart questions.": "ã‚ãªãŸã¯ã€è¦ç´„ã¨ã‚¹ãƒãƒ¼ãƒˆãªè³ªå•ã®æŠ½å‡ºã«ç‰¹åŒ–ã—ãŸAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚",
Â  Â  Â  Â  "You are a helpful AI assistant designed to answer questions based on the provided documents.\nAnalyze the following documents and answer the user's question as accurately and concisely as possible.\nIf the answer is not explicitly found in the documents, state that you cannot find the answer.": "ã‚ãªãŸã¯ã€æä¾›ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚ŒãŸã€å½¹ç«‹ã¤AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\nä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†æã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯èƒ½ãªé™ã‚Šæ­£ç¢ºã‹ã¤ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚\nç­”ãˆãŒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æ˜ç¤ºçš„ã«è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ç­”ãˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¨è¿°ã¹ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "Question:": "è³ªå•:",
Â  Â  Â  Â  "# DeloitteSmartâ„¢ AI Assistant Report\n\n## Document Summaries:\n": "# DeloitteSmartâ„¢ AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¬ãƒãƒ¼ãƒˆ\n\n## ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¦ç´„:\n",
Â  Â  Â  Â  "## Chat History:\n": "## ãƒãƒ£ãƒƒãƒˆå±¥æ­´:\n",
Â  Â  Â  Â  "Download Report": "ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
Â  Â  Â  Â  "Send": "é€ä¿¡",
Â  Â  Â  Â  "Please upload documents before asking questions.": "è³ªå•ã™ã‚‹å‰ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
Â  Â  Â  Â  "OpenAI API key is not available. Cannot generate summary.": "OpenAI APIã‚­ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚è¦ç´„ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã€‚",
Â  Â  Â  Â  "OpenAI API key is not available. Cannot answer questions.": "OpenAI APIã‚­ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚è³ªå•ã«ç­”ãˆã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚",
        "Processing document": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‡¦ç†ä¸­",
        "Generating summary and questions for": "ã®è¦ç´„ã¨è³ªå•ã‚’ç”Ÿæˆä¸­:",
        "OpenAI API key is pre-configured.": "OpenAI APIã‚­ãƒ¼ã¯äº‹å‰ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚",
        "OpenAI API key not found.": "OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚",
        "Powered by [Innov8]": "[Innov8] æä¾›",
        "Prototype Version 1.0": "ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ— ãƒãƒ¼ã‚¸ãƒ§ãƒ³ 1.0",
        "Secure | Scalable | Smart": "å®‰å…¨ | ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ« | ã‚¹ãƒãƒ¼ãƒˆ",
        "This document is large and has been split into chunks for analysis.": "ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã‚µã‚¤ã‚ºãŒå¤§ãã„ãŸã‚ã€åˆ†æã®ãŸã‚ã«åˆ†å‰²ã•ã‚Œã¾ã—ãŸã€‚",
    }
Â  Â  return translations.get(english_text, english_text) if language == "æ—¥æœ¬èª" else english_text

# --- HELPER FUNCTIONS ---

# Basic text splitting function to handle large documents
def split_text_into_chunks(text, chunk_size=SPLIT_CHUNK_SIZE):
    """Splits text by paragraphs, then joins into chunks of approximate size."""
    if not text:
        return []
    paragraphs = re.split(r'\n\s*\n', text) # Split by two or more newlines (paragraphs)
    chunks = []
    current_chunk = ""
    for para in paragraphs:
        # If adding the next paragraph exceeds the chunk size, save the current chunk
        # and start a new one. Add +2 for potential \n\n between paragraphs.
        if len(current_chunk) + len(para) + 2 > chunk_size and current_chunk:
            chunks.append(current_chunk.strip())
            current_chunk = para
        else:
            if current_chunk:
                current_chunk += "\n\n" + para
            else:
                current_chunk = para
    if current_chunk:
        chunks.append(current_chunk.strip())

    # Fallback split by character if paragraphs are too large
    final_chunks = []
    for chunk in chunks:
        if len(chunk) > chunk_size:
            # Simple character split if paragraph is too big
            sub_chunks = [chunk[i:i+chunk_size] for i in range(0, len(chunk), chunk_size)]
            final_chunks.extend(sub_chunks)
        else:
            final_chunks.append(chunk)

    return final_chunks


# --- SIDEBAR ---
# Set up the sidebar with logo, API key status, and branding
with st.sidebar:
Â  Â  st.image("deloitte_logo.png", width=200) # Assuming deloitte_logo.png is in the same directory
Â  Â  openai_api_key = st.secrets.get("OPENAI_API_KEY")
Â  Â  if openai_api_key:
Â  Â  Â  Â  st.sidebar.success(get_translation("âœ… OpenAI API key is pre-configured."))
Â  Â  else:
Â  Â  Â  Â  st.sidebar.error(get_translation("âŒ OpenAI API key not found."))
Â  Â  st.sidebar.markdown(get_translation("Powered by [Innov8]"))
Â  Â  st.sidebar.markdown(get_translation("Prototype Version 1.0"))
Â  Â  st.sidebar.markdown(get_translation("Secure | Scalable | Smart"))

# --- SESSION STATE SETUP ---
# Initialize session state variables if they don't exist
session_defaults = {
Â  Â  "chat_history": [], Â  # Stores the chat conversation
Â  Â  "user_question": "", # Temporary storage for user input
Â  Â  "feedback": [], Â  Â  Â # Stores submitted feedback
Â  Â  "document_content": {}, # Stores raw text content of uploaded documents {filename: text}
    "document_chunks": {}, # Stores processed chunks of documents {filename: [chunk1, chunk2, ...]}
Â  Â  "document_summary": {}, # Stores summaries {filename: summary_text}
Â  Â  "uploaded_filenames": [] # Tracks names of files already processed in this session
}
for key, default in session_defaults.items():
Â  Â  if key not in st.session_state:
Â  Â  Â  Â  st.session_state[key] = default

# --- TITLE ---
st.title(get_translation("DeloitteSmartâ„¢: Your AI Assistant for Faster, Smarter Decisions"))

# --- FILE UPLOAD ---
# Expander for file upload section
with st.expander(get_translation("ğŸ“ Upload Documents (PDF, TXT)")):
Â  Â  uploaded_files = st.file_uploader(
        get_translation("Upload Files"),
        type=["pdf", "txt"],
        accept_multiple_files=True,
        key="file_uploader" # Added a key
    )

    # Process uploaded files
Â  Â  if uploaded_files:
Â  Â  Â  Â  for file in uploaded_files:
Â  Â  Â  Â  Â  Â  filename = file.name
Â  Â  Â  Â  Â  Â  # Check if file has already been processed in this session
Â  Â  Â  Â  Â  Â  if filename not in st.session_state.uploaded_filenames:
Â  Â  Â  Â  Â  Â  Â  Â  # Add a spinner while processing each file
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(f"{get_translation('Processing document')} {filename}..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.uploaded_filenames.append(filename)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_bytes = file.getvalue() # Use getvalue() for BytesIO like object

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  document_text = ""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if file.type == "application/pdf":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Use BytesIO to read PDF from bytes
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with fitz.open(stream=file_bytes, filetype="pdf") as doc:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for page in doc:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  document_text += page.get_text() + "\n" # Add newline between pages
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif file.type == "text/plain":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  document_text = file_bytes.decode("utf-8")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.document_content[filename] = document_text.strip() # Store full content

                        # Split document into chunks for chat context (especially for large files)
                        st.session_state.document_chunks[filename] = split_text_into_chunks(
                            document_text, chunk_size=SPLIT_CHUNK_SIZE
                        )

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Summarize if API key is available
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if openai_api_key:
                            # Use the full text for summarization if not too large,
                            # otherwise use the first few chunks as context might be lost over chunks
                            summary_text_source = document_text if len(document_text) < MAX_CHUNK_CHARS else " ".join(st.session_state.document_chunks[filename][:5]) # Use first 5 chunks if very large

                            if summary_text_source:
        Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
        Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner(f"{get_translation('Generating summary and questions for')} {filename}..."):
            Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prompt = f"{get_translation('You are a highly trained consultant. Summarize the following content and generate 5 smart questions.')}\n\n{get_translation('Document:')}\n{summary_text_source}"
            Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  client = openai.OpenAI(api_key=openai_api_key)
            Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response = client.chat.completions.create(
            Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model="gpt-3.5-turbo", # Consider gpt-4 for better quality if available
            Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  messages=[
            Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"role": "system", "content": get_translation("You are an AI assistant specialized in summarizing and extracting smart questions.")},
            Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"role": "user", "content": prompt}
            Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
            Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
            Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.document_summary[filename] = response.choices[0].message.content
                                        if len(document_text) > MAX_CHUNK_CHARS:
                                            st.session_state.document_summary[filename] = (
                                                get_translation("This document is large and has been split into chunks for analysis.") +
                                                "\n\n" + st.session_state.document_summary[filename]
                                            )

        Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except OpenAIError as e:
        Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"{get_translation('Summary generation error for')} {filename}: {str(e)}")
                                except Exception as e:
        Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"{get_translation('Summary generation error for')} {filename}: An unexpected error occurred - {str(e)}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(get_translation("OpenAI API key is not available. Cannot generate summary."))

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Catch errors during file reading or initial processing
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"{get_translation('Error during document processing for')} {filename}: {str(e)}")

# --- SHOW SUMMARIES ---
# Display summaries if available in session state
if st.session_state.document_summary:
Â  Â  st.subheader(get_translation("ğŸ“„ Summaries & Smart Questions"))
Â  Â  for fname, summary in st.session_state.document_summary.items():
Â  Â  Â  Â  st.markdown(f"**ğŸ—‚ï¸ {fname}**")
        # Escape markdown characters to display summary raw
Â  Â  Â  Â  escaped_summary = summary.replace("#", "\\#").replace("*", "\\*").replace("_", "\\_").replace("`", "\\`").replace("[", "\\[").replace("]", "\\]")
Â  Â  Â  Â  st.markdown(escaped_summary)
Â  Â  Â  Â  st.markdown(get_translation("---")) # Separator

# --- CHAT INTERFACE ---
st.subheader(get_translation("ğŸ—£ï¸ Ask Questions Based on the Documents"))
chat_container = st.container() # Container to hold chat messages, allows easy clearing/scrolling

# Form for user chat input
with st.form("chat_input_form", clear_on_submit=True):
Â  Â  # Use columns for input text and send button
Â  Â  col1, col2 = st.columns([9, 1])
Â  Â  with col1:
Â  Â  Â  Â  # Text input for user's question
Â  Â  Â  Â  user_input = st.text_input(get_translation("Ask anything about the uploaded documents..."), key="user_input")
Â  Â  with col2:
Â  Â  Â  Â  # Add a bit of vertical space to align button
Â  Â  Â  Â  st.markdown("<div>&nbsp;</div>", unsafe_allow_html=True)
Â  Â  Â  Â  # Submit button for the form
Â  Â  Â  Â  submitted = st.form_submit_button(get_translation("Send"), use_container_width=True)

Â  Â  # Handle form submission
Â  Â  if submitted and user_input:
Â  Â  Â  Â  # Check if documents have been processed
Â  Â  Â  Â  if not st.session_state.document_chunks:
Â  Â  Â  Â  Â  Â  st.warning(get_translation("Please upload documents before asking questions."))
Â  Â  Â  Â  # Check if API key is available for chat
Â  Â  Â  Â  elif not openai_api_key:
Â  Â  Â  Â  Â  Â  st.warning(get_translation("OpenAI API key is not available. Cannot answer questions."))
Â  Â  Â  Â  else:
            # Prepare document context for the LLM by joining chunks (up to MAX_CHUNK_CHARS)
            all_chunks = [chunk for chunk_list in st.session_state.document_chunks.values() for chunk in chunk_list]
            combined_chunks_text = ""
            for chunk in all_chunks:
                # Add chunk if it fits within the MAX_CHUNK_CHARS limit
                if len(combined_chunks_text) + len(chunk) + 2 <= MAX_CHUNK_CHARS: # +2 for potential \n\n
                     if combined_chunks_text:
                         combined_chunks_text += "\n\n" + chunk
                     else:
                         combined_chunks_text = chunk
                else:
                    break # Stop adding chunks if the limit is reached

            if not combined_chunks_text:
                 st.warning("No document content available or chunks are too large.") # Should not happen if documents were processed

            # Construct the chat prompt with document context and user question
Â  Â  Â  Â  Â  Â  prompt_chat = f"""
{get_translation("You are a helpful AI assistant designed to answer questions based on the provided documents.\nAnalyze the following documents and answer the user's question as accurately and concisely as possible.\nIf the answer is not explicitly found in the documents, state that you cannot find the answer.")}

{get_translation("Documents:")}
{combined_chunks_text}

{get_translation("Question:")} {user_input}
"""
Â  Â  Â  Â  Â  Â  try:
                # Add user message to history immediately
                st.session_state.chat_history.append({"role": "user", "content": user_input})
                # Use st.status for ongoing process feedback
                with st.status("Generating response...", expanded=True) as status:
                    status.update(label="Sending query to AI...", state="running")
    Â  Â  Â  Â  Â  Â  Â  Â  client = openai.OpenAI(api_key=openai_api_key)
    Â  Â  Â  Â  Â  Â  Â  Â  response_chat = client.chat.completions.create(
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model="gpt-3.5-turbo", # Consider gpt-4 for better quality
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  messages=[
                            # Include chat history for context in conversation (optional, but good for persistent chat)
                            # Be mindful of token limits with long history
                            {"role": "system", "content": get_translation("You are an AI assistant that answers questions based on provided documents.")},
                            # Append previous chat history, excluding system messages
                            *[{"role": m["role"], "content": m["content"]} for m in st.session_state.chat_history[-10:] if m["role"] != "system"], # Include last 10 messages for context
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {"role": "user", "content": prompt_chat} # The current query including document context
    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
    Â  Â  Â  Â  Â  Â  Â  Â  )
    Â  Â  Â  Â  Â  Â  Â  Â  assistant_response = response_chat.choices[0].message.content
    Â  Â  Â  Â  Â  Â  Â  Â  # Append assistant's response to history
    Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append({"role": "assistant", "content": assistant_response})
                    status.update(label="Response received!", state="complete", icon="âœ…")

    Â  Â  Â  Â  Â  Â  except OpenAIError as e:
                status.update(label="Error!", state="error", icon="âŒ")
    Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"{get_translation('Error during chat completion:')} {str(e)}")
            except Exception as e:
                status.update(label="Error!", state="error", icon="âŒ")
                st.error(f"{get_translation('Error during chat completion:')} An unexpected error occurred - {str(e)}")


# --- DISPLAY CHAT HISTORY ---
# Display chat messages in the chat container
if st.session_state.chat_history:
Â  Â  with chat_container:
Â  Â  Â  Â  st.subheader(get_translation("ğŸ’¬ Chat History")) # Add subheader here
Â  Â  Â  Â  for message in st.session_state.chat_history:
            # Use st.chat_message for standard chat bubble styling
Â  Â  Â  Â  Â  Â  with st.chat_message(message["role"]):
                # Display role and content (role capitalized and translated)
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**{get_translation(message['role'].capitalize())}:** {message['content']}")

Â  Â  # Optional: Add JavaScript to attempt scrolling to the bottom of the chat history
Â  Â  # Note: This relies on internal Streamlit class names which might change
Â  Â  js = f"""
Â  Â  Â  Â  <script>
Â  Â  Â  Â  Â  Â  function scrollChatToBottom() {{
Â  Â  Â  Â  Â  Â  Â  Â  // Find the element containing the chat messages - class name might vary
Â  Â  Â  Â  Â  Â  Â  Â  const chatContainer = document.querySelector('[data-testid="chat-messages"]');
Â  Â  Â  Â  Â  Â  Â  Â  if (chatContainer) {{
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  chatContainer.scrollTop = chatContainer.scrollHeight;
Â  Â  Â  Â  Â  Â  Â  Â  }}
Â  Â  Â  Â  Â  Â  }}
Â  Â  Â  Â  Â  Â  // Use a slight delay to ensure elements are rendered
Â  Â  Â  Â  Â  Â  setTimeout(scrollChatToBottom, 100);
Â  Â  Â  Â  </script>
Â  Â  Â  Â  """
Â  Â  st.components.v1.html(js, height=0, width=0) # Embed the JS, hide the component output


# --- DOWNLOAD REPORT ---
# Provide a download link for the report if there's content
if st.session_state.chat_history or st.session_state.document_summary:
Â  Â  # Build the report content string
Â  Â  report_text = f"{get_translation('# DeloitteSmartâ„¢ AI Assistant Report')}\n\n"

    if st.session_state.document_summary:
        report_text += f"{get_translation('## Document Summaries')}:\n"
    Â  Â  for fname, summary in st.session_state.document_summary.items():
    Â  Â  Â  Â  report_text += f"### {fname}\n{summary}\n\n"

    if st.session_state.chat_history:
        report_text += f"\n{get_translation('## Chat History')}:\n"
    Â  Â  for chat in st.session_state.chat_history:
    Â  Â  Â  Â  report_text += f"**{get_translation(chat['role'].capitalize())}:** {chat['content']}\n\n"

Â  Â  # Function to create the download link (text file)
Â  Â  def create_download_link(report):
Â  Â  Â  Â  buffer = BytesIO()
Â  Â  Â  Â  buffer.write(report.encode("utf-8"))
Â  Â  Â  Â  buffer.seek(0)
        # Encode report content to base64 for embedding in the link
Â  Â  Â  Â  b64 = base64.b64encode(buffer.getvalue()).decode()
Â  Â  Â  Â  href = f'data:text/plain;base64,{b64}'
        # Generate a filename based on current time
Â  Â  Â  Â  filename = f"deloitte_smart_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
Â  Â  Â  Â  return f'<a href="{href}" download="{filename}">{get_translation("Download Report")}</a>'

Â  Â  st.markdown(get_translation("---")) # Separator
Â  Â  st.subheader(get_translation("â¬‡ï¸ Download Report"))
    # Display the generated download link
Â  Â  st.markdown(create_download_link(report_text), unsafe_allow_html=True)

# --- FEEDBACK SECTION ---
st.markdown(get_translation("---")) # Separator
st.subheader(get_translation("ğŸ“ Feedback"))
# Text area for user feedback
feedback_text = st.text_area(
    get_translation("Share your feedback to help us improve:"),
    key="feedback_input", # Key for session state
    height=100
)
# Button to submit feedback
if st.button(get_translation("Submit Feedback")):
Â  Â  if feedback_text:
Â  Â  Â  Â  # Store feedback with timestamp
Â  Â  Â  Â  feedback_entry = {"timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"), "feedback": feedback_text}
Â  Â  Â  Â  st.session_state.feedback.append(feedback_entry)
Â  Â  Â  Â  st.success(get_translation("Thank you for your feedback!"))
        # Clear the feedback text area after submission
Â  Â  Â  Â  st.session_state.feedback_input = "" # Clear the widget via its key
Â  Â  else:
Â  Â  Â  Â  st.warning(get_translation("Please enter your feedback."))

# Display submitted feedback
if st.session_state.feedback:
Â  Â  st.subheader(get_translation("ğŸ“¬ Submitted Feedback"))
Â  Â  for fb in st.session_state.feedback:
Â  Â  Â  Â  st.markdown(f"**{get_translation('Timestamp')}:** {fb['timestamp']}")
Â  Â  Â  Â  st.markdown(fb['feedback'])
Â  Â  Â  Â  st.markdown(get_translation("---")) # Separator for feedback entries
